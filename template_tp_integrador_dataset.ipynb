{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27719223",
   "metadata": {},
   "source": [
    "\n",
    "# Trabajo Práctico 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "461d011d",
   "metadata": {},
   "source": [
    "Dentro de la carpeta de `data/` vamos a poder encontrar 3 datasets:\n",
    "\n",
    "- Pokemon (Problema de clasificacion Binaria): La intención es tratar de predecir si basada en la información que tenemos podemos estimar si la carta que estamos viendo es Legendaria o no (Booleano).\n",
    "- Heart Disease (Problema de clasificacion Binaria): A partir de este dataset vamos a tratar de determinar si una persona basado en ciertos datos tiene asociado una enfermedad cardiaca.\n",
    "- Challenger USA Space Shuttle O-Ring Data Set (Regresion multivariada): En este dataset encontramos información sobre varios despegues del space shuttle, la intención de este problema es tratar de predecir cuanto será la cantidad de O-Rings que fallan dadas las condiciones de despegue.\n",
    "\n",
    "> Para la presentación de sus trabajos prácticos **Elijan 1 de los 3 datasets**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a976dd3",
   "metadata": {},
   "source": [
    "# Importar librerías\n",
    "\n",
    "Importar aquellas librerías que serán utilizadas en el trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "845b6c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA,TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV, LogisticRegression, LogisticRegressionCV, SGDClassifier, SGDRegressor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "954111ce",
   "metadata": {},
   "source": [
    "# Cargar datos\n",
    "\n",
    "Cargar los datos de entrenamiento.\n",
    "\n",
    "Recordemos que para todos los datasets tenemos el archivo `columns.csv` que contiene el nombre y descripción de cada columna en el mismo orden que van a encontrar los datos.\n",
    "\n",
    "En los casos de datos con extensión  `*.data` vamos a tener que leerlo utilizando `pd.read_data('myfile.data', sep = ',', header = 0, names = my_cols)`\n",
    "\n",
    "De esta manera si quisieramos cargar datos por ejemplo de Nasa deberiamos poner algo asi\n",
    "\n",
    "\n",
    "```python\n",
    "## Importing python earlier\n",
    "\n",
    "my_cols = pd.read_csv('./data/nasa_orings/columns.csv').column.values.tolist()\n",
    "\n",
    "my_data = pd.read_csv('./data/nasa_orings/o-ring-erosion-or-blowby.data',\n",
    "                       delimiter=',',\n",
    "                       header=0,\n",
    "                       names = my_cols\n",
    "                    )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26f41c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cols = pd.read_csv('./data/pokemon/columns.csv').column.values.tolist()\n",
    "\n",
    "my_data = pd.read_csv('./data/pokemon/pokemon.csv',\n",
    "                       delimiter=',',\n",
    "                       header=0,\n",
    "                       names = my_cols\n",
    "                    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c93000bd",
   "metadata": {},
   "source": [
    "# Análisis exploratorio básico y preprocesamiento de los datos\n",
    "\n",
    "Análisis de los datos para conocer los mismos, ver datos faltantes, decidir cómo tratarlos, ver distribuciones, relaciones, etc. Procesar los datos centrándolos, reescalando, codificando, reduciendo dimensiones, etc. según considere necesario.\n",
    "\n",
    "Vamos a considerar:\n",
    "\n",
    "- Para escalar datos: `sklearn.preprocessing.StandardScaler()`\n",
    "- Para completar valores nulos: `sklearn.impute.SimpleImputer()` [Quickguide](https://scikit-learn.org/stable/modules/impute.html)\n",
    "- Para reduccion de dimensiones (sobre todo para visualizar los datos): PCA o [T-SNE](https://builtin.com/data-science/tsne-python) (Solo para visualización)\n",
    "- Para codificar variables categóricas: `sklearn.preprocessing.OneHotEncoder()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b66211cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 801 entries, (\"['Overgrow', 'Chlorophyll']\", 1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 2.0, 2.0, 1.0, 0.25, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0) to (\"['Soul-Heart']\", 0.25, 0.5, 0.0, 1.0, 0.5, 1.0, 2.0, 0.5, 1.0, 0.5, 2.0, 0.5, 0.5, 0.0, 0.5, 0.5, 1.0)\n",
      "Data columns (total 23 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   name               801 non-null    float64\n",
      " 1   japanese_name      801 non-null    int64  \n",
      " 2   pokedex_number     801 non-null    int64  \n",
      " 3   percentage_male    801 non-null    int64  \n",
      " 4   type1              801 non-null    int64  \n",
      " 5   type2              801 non-null    object \n",
      " 6   classification     801 non-null    object \n",
      " 7   height_m           801 non-null    int64  \n",
      " 8   weight_kg          801 non-null    int64  \n",
      " 9   capture_rate       781 non-null    float64\n",
      " 10  baseeggsteps       801 non-null    int64  \n",
      " 11  abilities          801 non-null    object \n",
      " 12  experience_growth  801 non-null    object \n",
      " 13  base_happiness     703 non-null    float64\n",
      " 14  against_?          801 non-null    int64  \n",
      " 15  hp                 801 non-null    int64  \n",
      " 16  attack             801 non-null    int64  \n",
      " 17  defense            801 non-null    int64  \n",
      " 18  sp_attack          801 non-null    object \n",
      " 19  sp_defense         417 non-null    object \n",
      " 20  speed              781 non-null    float64\n",
      " 21  generation         801 non-null    int64  \n",
      " 22  is_legendary       801 non-null    int64  \n",
      "dtypes: float64(4), int64(13), object(6)\n",
      "memory usage: 182.5+ KB\n",
      "None\n",
      "             name  japanese_name  pokedex_number  percentage_male       type1  \\\n",
      "count  801.000000     801.000000      801.000000       801.000000  801.000000   \n",
      "mean     1.058365      77.857678     7191.011236        65.362047  428.377029   \n",
      "std      0.606562      32.158820     6558.220422        19.598948  119.203577   \n",
      "min      0.250000       5.000000     1280.000000         0.000000  180.000000   \n",
      "25%      0.500000      55.000000     5120.000000        70.000000  320.000000   \n",
      "50%      1.000000      75.000000     5120.000000        70.000000  435.000000   \n",
      "75%      1.000000     100.000000     6400.000000        70.000000  505.000000   \n",
      "max      4.000000     185.000000    30720.000000       140.000000  780.000000   \n",
      "\n",
      "         height_m     weight_kg  capture_rate  baseeggsteps  base_happiness  \\\n",
      "count  801.000000  8.010000e+02    781.000000    801.000000      703.000000   \n",
      "mean    73.008739  1.054996e+06      1.163892     68.958801       55.155761   \n",
      "std     30.769159  1.602558e+05      1.080326     26.576015       20.261623   \n",
      "min      5.000000  6.000000e+05      0.100000      1.000000        0.000000   \n",
      "25%     50.000000  1.000000e+06      0.600000     50.000000       50.000000   \n",
      "50%     70.000000  1.000000e+06      1.000000     65.000000       50.000000   \n",
      "75%     90.000000  1.059860e+06      1.500000     80.000000       50.000000   \n",
      "max    230.000000  1.640000e+06     14.500000    255.000000      100.000000   \n",
      "\n",
      "        against_?          hp      attack     defense       speed  generation  \\\n",
      "count  801.000000  801.000000  801.000000  801.000000  781.000000  801.000000   \n",
      "mean   401.000000   71.305868   70.911361   66.334582   61.378105    3.690387   \n",
      "std    231.373075   32.353826   27.942501   28.907662  109.354766    1.930420   \n",
      "min      1.000000   10.000000   20.000000    5.000000    0.100000    1.000000   \n",
      "25%    201.000000   45.000000   50.000000   45.000000    9.000000    2.000000   \n",
      "50%    401.000000   65.000000   66.000000   65.000000   27.300000    4.000000   \n",
      "75%    601.000000   91.000000   90.000000   85.000000   64.800000    5.000000   \n",
      "max    801.000000  194.000000  230.000000  180.000000  999.900000    7.000000   \n",
      "\n",
      "       is_legendary  \n",
      "count    801.000000  \n",
      "mean       0.087391  \n",
      "std        0.282583  \n",
      "min        0.000000  \n",
      "25%        0.000000  \n",
      "50%        0.000000  \n",
      "75%        0.000000  \n",
      "max        1.000000  \n"
     ]
    }
   ],
   "source": [
    "# Análisis exploratorio básico\n",
    "print(my_data.info())\n",
    "print(my_data.describe())\n",
    "\n",
    "# Crear listas de tipos de columnas\n",
    "num_cols = my_data.select_dtypes(include=['int64', 'float64']).columns\n",
    "cat_cols = my_data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Preprocesamiento\n",
    "# Reemplazar los valores nulos en las columnas numéricas con la mediana y escalar\n",
    "num_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# Reemplazar los valores nulos en las columnas categóricas con la moda y one-hot encode\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Juntar los transformadores\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, num_cols),\n",
    "        ('cat', cat_transformer, cat_cols)])\n",
    "\n",
    "# Ajustar y transformar los datos\n",
    "df_preprocessed = preprocessor.fit_transform(my_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06f7094a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X = df_preprocessed\n",
    "y = my_data['is_legendary']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Asumamos que 'X' es tu conjunto de datos\n",
    "# supongamos que X_sparse es tu matriz de datos dispersos\n",
    "#svd = TruncatedSVD(n_components=50)\n",
    "#X_reduced = svd.fit_transform(X)\n",
    "#pca = PCA(n_components=2)  # Reducir a 2 componentes\n",
    "#X_pca = pca.fit_transform(X_train)\n",
    "\n",
    "# Entrenar el modelo\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predecir\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "285a3eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Recall: 1.0\n",
      "Precision: 1.0\n",
      "F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Recall:', recall_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred))\n",
    "print('F1 Score:', f1_score(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13e49832",
   "metadata": {},
   "source": [
    "# Modelos y evaluación\n",
    "Probar diferentes modelos para predecir la variable objetivo. Calcular las métricas que considere relevantes. Comentar los resultados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "529db73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, ElasticNetCV, LogisticRegression, LogisticRegressionCV, SGDClassifier, SGDRegressor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "23cb2388443d556543181094e3ae896af451ee7d366981f58b81df73afb487db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
