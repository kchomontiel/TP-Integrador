{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "07289bb0",
      "metadata": {},
      "source": [
        "# Trabajo práctico integrador\n",
        "\n",
        "**Nombre**:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "afa76473",
      "metadata": {},
      "source": [
        "## Primera Parte (Clase 1 y 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaf94e0a",
      "metadata": {
        "id": "aaf94e0a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "0ffe9554",
      "metadata": {},
      "source": [
        "### Primer ejercicio\n",
        "\n",
        "Dada una matriz en formato *numpy array*, donde cada fila de la matriz representa un vector matemático, se requiere computar las normas $l_0$, $l_1$, $l_2$, $l_{\\infty}$, según la siguientes definiciones:\n",
        "\n",
        "\\begin{equation}\n",
        "    ||\\mathbf{x}||^{p} = \\bigg(\\sum_{j=1}^{n}{|x_i|^p}\\bigg)^{\\frac{1}{p}}\n",
        "\\end{equation}\n",
        "\n",
        "con los casos especiales para $p=0$ y $p=\\infty$ siendo:\n",
        "\n",
        "\\begin{equation}\n",
        "    \\begin{array}{rcl}\n",
        "        ||\\mathbf{x}||_0 & = & \\bigg(\\sum_{j=1 \\wedge x_j != 0}{|x_i|}\\bigg)\\\\\n",
        "        ||\\mathbf{x}||_{\\infty} & = & \\max_{i}{|x_i|}\\\\\n",
        "    \\end{array}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bdb0ee3",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "dd66d862",
      "metadata": {},
      "source": [
        "### Segundo Ejercicio\n",
        "\n",
        "En clasificación contamos con dos arreglos, la “verdad” y la “predicción”. Cada elemento de los arreglos pueden tomar dos valores, “True” (representado por 1) y “False” (representado por 0). Entonces podemos definir 4 variables:\n",
        "\n",
        "* True Positive (TP): El valor verdadero es 1 y el valor predicho es 1\n",
        "* True Negative (TN): El valor verdadero es 0 y el valor predicho es 0\n",
        "* False Positive (FP): El valor verdadero es 0 y el valor predicho es 1\n",
        "* False Negative (FN): El valor verdadero es 1 y el valor predicho es 0\n",
        "\n",
        "A partir de esto definimos:\n",
        "\n",
        "* Precision = TP / (TP + FP)\n",
        "* Recall = TP / (TP + FN)\n",
        "* Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        " \n",
        "Calcular las 3 métricas con Numpy y operaciones vectorizadas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "794dcd58",
      "metadata": {},
      "outputs": [],
      "source": [
        "truth = np.array([1,1,0,1,1,1,0,0,0,1])\n",
        "prediction = np.array([1,1,1,1,0,0,1,1,0,0])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d0ac6f6f",
      "metadata": {},
      "source": [
        "### Tercer ejercicio\n",
        "\n",
        "Crear una función que separe los datos en train-validation-test. Debe recibir de parametros:\n",
        "\n",
        "- X: Array o Dataframe que contiene los datos de entrada del sistema.\n",
        "- y: Array o Dataframe que contiene la(s) variable(s) target del problema.\n",
        "- train_percentage: _float_ el porcentaje de training.\n",
        "- test_percentage: _float_ el porcentaje de testing.\n",
        "- val_percentage: _float_ el porcentaje de validación.\n",
        "- shuffle: _bool_ determina si el split debe hacerse de manera random o no.\n",
        "\n",
        "Hints: \n",
        "\n",
        "* Usar Indexing y slicing\n",
        "* Usar np.random.[...]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca85fc66",
      "metadata": {},
      "outputs": [],
      "source": [
        "def split(X_input,\n",
        "          Y_input,\n",
        "          train_size=0.7,\n",
        "          val_size=0.15,\n",
        "          test_size=0.15,\n",
        "          random_state=42,\n",
        "          shuffle=True):\n",
        "    \n",
        "    return NotImplementedError"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cdaf7bba",
      "metadata": {},
      "source": [
        "## Segunda parte (Aprendizaje Supervisado)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "dc5e74de",
      "metadata": {},
      "source": [
        "Para la creación de los datasets y la manipulación de los mismos vamos a trabajar directamente con dos módulos includios en la carpeta utils.\n",
        "\n",
        "En esta podemos encontrar:\n",
        " - generate_data: Esta función wrappea el método de _make_regression_ de scikit learn para devolver un dataframe con un problema de regresión basado en sus parámetros.\n",
        " - generate_outliers: Esta función genera outliers livianos y pesados en función de los parámetros que le demos de entrada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a033541",
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.data_generation import generate_dataset\n",
        "from utils.data_manipulation import generate_outliers"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "2734dfde",
      "metadata": {},
      "source": [
        "### Cuarto ejercicio\n",
        "\n",
        "Utilizando la funcion `generate_data` generar un problema de regresión multivariada en el cual cuente con N variables informativas y M variables no informativas.\n",
        "\n",
        "Ejemplo:\n",
        "```python\n",
        "data = generate_dataset(n_samples=1000,\n",
        "                    n_features=10,\n",
        "                    n_informative=5,\n",
        "                    n_targets=1,\n",
        "                    noise=20.0,\n",
        "                    random_state=42,\n",
        "                    output='dataframe')\n",
        "\n",
        "```\n",
        "\n",
        "Analice como varía el problema cuando se generan múltiples veces el dataset con un valor de _noise_ fijo. \n",
        "\n",
        "- Qué pasa con los coeficientes de las variables no informativas?\n",
        "- La regresión se ve afectada por estas variables?\n",
        "- Simule el mismo dataset 100 veces y analice los coeficientes, que se puede notar?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4d4420c8",
      "metadata": {},
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5772f3b8",
      "metadata": {},
      "source": [
        "### Quinto ejercicio\n",
        "\n",
        "Utilizando la funcion `generate_outliers` generar puntos extremos dentro de los datos que generamos anteriormente. En este ejercicio dejar setteado `extreme_outliers` como `False` y observe como variando el porcentaje de los mismos la regresión comienza a afectarse.\n",
        "\n",
        "Pasos:\n",
        "\n",
        "1. Generar datasets:\n",
        "    - Uno normal con poco `noise` y pocos outliers\n",
        "    - Uno con mucho `noise` y pocos outliers\n",
        "    - Uno con poco `noise` y muchos outliers\n",
        "    - Uno con mucho `noise` y muchos outliers\n",
        "2. Probar los distintos regresores a ver como se comportan frente a estos datasets anómalos.\n",
        "3. Comparar y analizar resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82a019af",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression, HuberRegressor, ElasticNetCV"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5bcb8881",
      "metadata": {},
      "source": [
        "## Tercera Parte (No supervisado)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f5921336",
      "metadata": {},
      "source": [
        "### Sexto y Septimo Ejercicio\n",
        "\n",
        "Para este ejercicio vamos a considerar los siguientes datasets:\n",
        "\n",
        "* [HAR](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) (Ejercicio 3)\n",
        "* [MNIST](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html) (Ejercicio 4)\n",
        "\n",
        "1. Aplicar PCA (validar que se cumplan las condiciones), ¿Cuántas componentes necesitamos para explicar el 80% de la varianza?\n",
        "2. Gráficar la variación acumulada para cada caso.\n",
        "3. Utilizando [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html). Agrupar el dataset transformado (ejercicio de PCA) y agrupar en clusters de $k=6$ (ej 6) y $k=10$ (ej 7). Luego en ambos casos probar con $k=2$.\n",
        "4. Graficar los resultados con los distintos k's usando las primeras dos componentes principales como ejes x,y.\n",
        "5. Explique. ¿Cuál fue la ganancia de usar PCA en conjunto con k-means?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Entrenamiento de modelos de prueba\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Evaluación de modelos de prueba\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Crear datasets\n",
        "from sklearn.datasets import make_regression"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3d4de736",
      "metadata": {},
      "source": [
        "## Ejercicio 8\n",
        "\n",
        "TBD"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ee6d1602",
      "metadata": {},
      "source": [
        "## Cuarta Parte"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ce61f3a7",
      "metadata": {},
      "source": [
        "La ultima parte la van a poder encontrar en el archivo `template_tp_integrador_tercera_parte.ipynb`"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Practica_clase_3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "b5c22da4a52024410f64f9c5a5e2b4ffeeb944a5ed00e8825a42174cdab30315"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
